{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 0.01                  \n",
    "EPSILON = 0.9              \n",
    "GAMMA = 0.9                 \n",
    "TARGET_REPLACE_ITER = 100   \n",
    "MEMORY_CAPACITY = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_actions = env.action_space.n\n",
    "max_states = env.observation_space.shape[0]\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(max_states, 50)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   \n",
    "        self.out = nn.Linear(50, max_actions)\n",
    "        self.out.weight.data.normal_(0, 0.1)   \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        self.learn_step_counter = 0                                     \n",
    "        \n",
    "        self.memory_counter = 0                                         \n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, 2*max_states + 2))     \n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n",
    "\n",
    "        if np.random.uniform() < EPSILON:  \n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  \n",
    "        else:   \n",
    "            action = np.random.randint(0, max_actions)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        \n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        \n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :max_states])\n",
    "        b_a = torch.LongTensor(b_memory[:, max_states:max_states+1].astype(int))\n",
    "        b_r = torch.FloatTensor(b_memory[:, max_states+1:max_states+2])\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -max_states:])\n",
    "\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  \n",
    "        q_next = self.target_net(b_s_).detach()     \n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   \n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  205 , Episode Reward:  1.23\n",
      "Episode:  206 , Episode Reward:  4.81\n",
      "Episode:  207 , Episode Reward:  13.67\n",
      "Episode:  208 , Episode Reward:  2.55\n",
      "Episode:  209 , Episode Reward:  1.38\n",
      "Episode:  210 , Episode Reward:  1.94\n",
      "Episode:  211 , Episode Reward:  1.08\n",
      "Episode:  212 , Episode Reward:  5.28\n",
      "Episode:  213 , Episode Reward:  5.95\n",
      "Episode:  214 , Episode Reward:  3.03\n",
      "Episode:  215 , Episode Reward:  2.45\n",
      "Episode:  216 , Episode Reward:  2.6\n",
      "Episode:  217 , Episode Reward:  6.43\n",
      "Episode:  218 , Episode Reward:  2.11\n",
      "Episode:  219 , Episode Reward:  18.12\n",
      "Episode:  220 , Episode Reward:  9.74\n",
      "Episode:  221 , Episode Reward:  28.43\n",
      "Episode:  222 , Episode Reward:  54.87\n",
      "Episode:  223 , Episode Reward:  15.22\n",
      "Episode:  224 , Episode Reward:  93.72\n",
      "Episode:  225 , Episode Reward:  58.09\n",
      "Episode:  226 , Episode Reward:  106.95\n",
      "Episode:  227 , Episode Reward:  174.89\n",
      "Episode:  228 , Episode Reward:  183.86\n",
      "Episode:  229 , Episode Reward:  73.81\n",
      "Episode:  230 , Episode Reward:  65.39\n",
      "Episode:  231 , Episode Reward:  140.37\n",
      "Episode:  232 , Episode Reward:  125.66\n",
      "Episode:  233 , Episode Reward:  108.22\n",
      "Episode:  234 , Episode Reward:  140.08\n",
      "Episode:  235 , Episode Reward:  179.83\n",
      "Episode:  236 , Episode Reward:  205.09\n",
      "Episode:  237 , Episode Reward:  402.63\n",
      "Episode:  238 , Episode Reward:  569.16\n",
      "Episode:  239 , Episode Reward:  450.19\n",
      "Episode:  240 , Episode Reward:  787.38\n",
      "Episode:  241 , Episode Reward:  187.78\n",
      "Episode:  242 , Episode Reward:  267.19\n",
      "Episode:  243 , Episode Reward:  393.4\n",
      "Episode:  244 , Episode Reward:  243.95\n",
      "Episode:  245 , Episode Reward:  597.7\n",
      "Episode:  246 , Episode Reward:  247.81\n",
      "Episode:  247 , Episode Reward:  250.87\n",
      "Episode:  248 , Episode Reward:  153.18\n",
      "Episode:  249 , Episode Reward:  191.16\n",
      "Episode:  250 , Episode Reward:  247.36\n",
      "Episode:  251 , Episode Reward:  157.47\n",
      "Episode:  252 , Episode Reward:  1320.39\n",
      "Episode:  253 , Episode Reward:  403.89\n",
      "Episode:  254 , Episode Reward:  837.18\n",
      "Episode:  255 , Episode Reward:  604.22\n",
      "Episode:  256 , Episode Reward:  348.94\n",
      "Episode:  257 , Episode Reward:  129.99\n",
      "Episode:  258 , Episode Reward:  629.66\n",
      "Episode:  259 , Episode Reward:  296.29\n",
      "Episode:  260 , Episode Reward:  247.47\n",
      "Episode:  261 , Episode Reward:  141.68\n",
      "Episode:  262 , Episode Reward:  206.65\n",
      "Episode:  263 , Episode Reward:  364.5\n",
      "Episode:  264 , Episode Reward:  609.7\n",
      "Episode:  265 , Episode Reward:  349.32\n",
      "Episode:  266 , Episode Reward:  455.08\n",
      "Episode:  267 , Episode Reward:  278.98\n",
      "Episode:  268 , Episode Reward:  273.89\n",
      "Episode:  269 , Episode Reward:  440.2\n",
      "Episode:  270 , Episode Reward:  748.23\n",
      "Episode:  271 , Episode Reward:  266.75\n",
      "Episode:  272 , Episode Reward:  199.36\n",
      "Episode:  273 , Episode Reward:  907.0\n",
      "Episode:  274 , Episode Reward:  125.87\n",
      "Episode:  275 , Episode Reward:  877.03\n",
      "Episode:  276 , Episode Reward:  299.64\n",
      "Episode:  277 , Episode Reward:  1272.12\n",
      "Episode:  278 , Episode Reward:  377.74\n",
      "Episode:  279 , Episode Reward:  592.8\n",
      "Episode:  280 , Episode Reward:  591.57\n",
      "Episode:  281 , Episode Reward:  234.64\n",
      "Episode:  282 , Episode Reward:  397.6\n",
      "Episode:  283 , Episode Reward:  198.23\n",
      "Episode:  284 , Episode Reward:  479.19\n",
      "Episode:  285 , Episode Reward:  407.33\n",
      "Episode:  286 , Episode Reward:  141.67\n",
      "Episode:  287 , Episode Reward:  157.36\n",
      "Episode:  288 , Episode Reward:  388.44\n",
      "Episode:  289 , Episode Reward:  372.84\n",
      "Episode:  290 , Episode Reward:  469.3\n",
      "Episode:  291 , Episode Reward:  1248.94\n",
      "Episode:  292 , Episode Reward:  427.71\n",
      "Episode:  293 , Episode Reward:  386.35\n",
      "Episode:  294 , Episode Reward:  239.35\n",
      "Episode:  295 , Episode Reward:  282.73\n",
      "Episode:  296 , Episode Reward:  375.5\n",
      "Episode:  297 , Episode Reward:  501.04\n",
      "Episode:  298 , Episode Reward:  271.13\n",
      "Episode:  299 , Episode Reward:  397.07\n",
      "Episode:  300 , Episode Reward:  252.64\n",
      "Episode:  301 , Episode Reward:  322.85\n",
      "Episode:  302 , Episode Reward:  284.13\n",
      "Episode:  303 , Episode Reward:  420.8\n",
      "Episode:  304 , Episode Reward:  232.96\n",
      "Episode:  305 , Episode Reward:  156.61\n",
      "Episode:  306 , Episode Reward:  955.53\n",
      "Episode:  307 , Episode Reward:  261.43\n",
      "Episode:  308 , Episode Reward:  154.79\n",
      "Episode:  309 , Episode Reward:  415.07\n",
      "Episode:  310 , Episode Reward:  306.21\n",
      "Episode:  311 , Episode Reward:  813.81\n",
      "Episode:  312 , Episode Reward:  699.18\n",
      "Episode:  313 , Episode Reward:  374.53\n",
      "Episode:  314 , Episode Reward:  223.22\n",
      "Episode:  315 , Episode Reward:  666.36\n",
      "Episode:  316 , Episode Reward:  741.83\n",
      "Episode:  317 , Episode Reward:  474.9\n",
      "Episode:  318 , Episode Reward:  942.07\n",
      "Episode:  319 , Episode Reward:  735.49\n",
      "Episode:  320 , Episode Reward:  185.0\n",
      "Episode:  321 , Episode Reward:  178.43\n",
      "Episode:  322 , Episode Reward:  438.58\n",
      "Episode:  323 , Episode Reward:  630.49\n",
      "Episode:  324 , Episode Reward:  216.69\n",
      "Episode:  325 , Episode Reward:  325.03\n",
      "Episode:  326 , Episode Reward:  152.78\n",
      "Episode:  327 , Episode Reward:  286.52\n",
      "Episode:  328 , Episode Reward:  154.46\n",
      "Episode:  329 , Episode Reward:  803.81\n",
      "Episode:  330 , Episode Reward:  875.68\n",
      "Episode:  331 , Episode Reward:  695.6\n",
      "Episode:  332 , Episode Reward:  707.06\n",
      "Episode:  333 , Episode Reward:  369.72\n",
      "Episode:  334 , Episode Reward:  739.28\n",
      "Episode:  335 , Episode Reward:  218.27\n",
      "Episode:  336 , Episode Reward:  638.92\n",
      "Episode:  337 , Episode Reward:  756.14\n",
      "Episode:  338 , Episode Reward:  569.25\n",
      "Episode:  339 , Episode Reward:  310.9\n",
      "Episode:  340 , Episode Reward:  162.19\n",
      "Episode:  341 , Episode Reward:  266.26\n",
      "Episode:  342 , Episode Reward:  226.32\n",
      "Episode:  343 , Episode Reward:  261.24\n",
      "Episode:  344 , Episode Reward:  476.96\n",
      "Episode:  345 , Episode Reward:  391.38\n",
      "Episode:  346 , Episode Reward:  259.7\n",
      "Episode:  347 , Episode Reward:  1646.21\n",
      "Episode:  348 , Episode Reward:  1538.32\n",
      "Episode:  349 , Episode Reward:  517.01\n",
      "Episode:  350 , Episode Reward:  600.97\n",
      "Episode:  351 , Episode Reward:  382.51\n",
      "Episode:  352 , Episode Reward:  1597.89\n",
      "Episode:  353 , Episode Reward:  544.57\n",
      "Episode:  354 , Episode Reward:  571.05\n",
      "Episode:  355 , Episode Reward:  219.32\n",
      "Episode:  356 , Episode Reward:  125.73\n",
      "Episode:  357 , Episode Reward:  280.24\n",
      "Episode:  358 , Episode Reward:  511.2\n",
      "Episode:  359 , Episode Reward:  813.31\n",
      "Episode:  360 , Episode Reward:  296.54\n",
      "Episode:  361 , Episode Reward:  517.04\n",
      "Episode:  362 , Episode Reward:  235.66\n",
      "Episode:  363 , Episode Reward:  903.07\n",
      "Episode:  364 , Episode Reward:  281.35\n",
      "Episode:  365 , Episode Reward:  234.02\n",
      "Episode:  366 , Episode Reward:  687.62\n",
      "Episode:  367 , Episode Reward:  472.31\n",
      "Episode:  368 , Episode Reward:  900.21\n",
      "Episode:  369 , Episode Reward:  294.09\n",
      "Episode:  370 , Episode Reward:  327.57\n",
      "Episode:  371 , Episode Reward:  306.61\n",
      "Episode:  372 , Episode Reward:  347.82\n",
      "Episode:  373 , Episode Reward:  25.54\n",
      "Episode:  374 , Episode Reward:  3.46\n",
      "Episode:  375 , Episode Reward:  84.24\n",
      "Episode:  376 , Episode Reward:  126.83\n",
      "Episode:  377 , Episode Reward:  117.79\n",
      "Episode:  378 , Episode Reward:  209.9\n",
      "Episode:  379 , Episode Reward:  394.3\n",
      "Episode:  380 , Episode Reward:  297.4\n",
      "Episode:  381 , Episode Reward:  553.21\n",
      "Episode:  382 , Episode Reward:  202.74\n",
      "Episode:  383 , Episode Reward:  171.06\n",
      "Episode:  384 , Episode Reward:  469.51\n",
      "Episode:  385 , Episode Reward:  752.81\n",
      "Episode:  386 , Episode Reward:  1083.21\n",
      "Episode:  387 , Episode Reward:  288.41\n",
      "Episode:  388 , Episode Reward:  375.71\n",
      "Episode:  389 , Episode Reward:  507.68\n",
      "Episode:  390 , Episode Reward:  109.56\n",
      "Episode:  391 , Episode Reward:  864.3\n",
      "Episode:  392 , Episode Reward:  1150.79\n",
      "Episode:  393 , Episode Reward:  199.25\n",
      "Episode:  394 , Episode Reward:  197.74\n",
      "Episode:  395 , Episode Reward:  743.93\n",
      "Episode:  396 , Episode Reward:  603.61\n",
      "Episode:  397 , Episode Reward:  14.13\n",
      "Episode:  398 , Episode Reward:  84.26\n",
      "Episode:  399 , Episode Reward:  173.39\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(400):\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    \n",
    "    while True:\n",
    "        a = dqn.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        \n",
    "        r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        r = r1 + r2\n",
    "\n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "\n",
    "        ep_r += r\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            if done:\n",
    "                print('Episode:', i_episode, ', Episode Reward:', round(ep_r, 2))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
